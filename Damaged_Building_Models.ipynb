{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90398a47-b79e-4b07-b24b-b4b3817ccee2",
   "metadata": {},
   "source": [
    "# Machine Learning Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f95bdd3-3fb2-4d8a-894c-846359736613",
   "metadata": {},
   "source": [
    "## Construction of Test/Train datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aba25ea3-b043-4fe3-890f-333d91190545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33/3444650230.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Let's make sure these directories are clean before we start\n",
    "import shutil\n",
    "try:\n",
    "    shutil.rmtree(\"../data/project3/data_all_modified/data_split/train\")\n",
    "    shutil.rmtree(\"../data/project3/data_all_modified/data_split/test\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f0a43b1-a99f-46d1-a3a9-4f0a5ddad469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have two classes which contains all the data: Damage & No_damage\n",
    "# Let's create directories for each class in the train and test directories.\n",
    "import os\n",
    "# ensure directories exist\n",
    "from pathlib import Path\n",
    "\n",
    "Path(\"../data/project3/data_all_modified/data_split/test/damage\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"../data/project3/data_all_modified/data_split/test/no_damage\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "Path(\"../data/project3/data_all_modified/data_split/train/damage\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"../data/project3/data_all_modified/data_split/train/no_damage\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b29497d8-3b2f-409d-b9c7-58b409a1797f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need paths of images for individual classes so we can copy them in the new directories that we created above\n",
    "all_damage_file_paths = os.listdir('../data/project3/data_all_modified/damage')\n",
    "all_no_damage_file_paths = os.listdir('../data/project3/data_all_modified/no_damage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8175434d-c0e7-489c-927e-490c0fd9bbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Damage image count:  11336\n",
      "test Damage image count:  2834\n",
      "len of overlap:  0\n",
      "train No Damage image count:  5721\n",
      "test No Damage image count:  1431\n",
      "len of overlap:  0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "train_damage_paths = random.sample(all_damage_file_paths, int(len(all_damage_file_paths)*0.8))\n",
    "print(\"train Damage image count: \", len(train_damage_paths))\n",
    "test_damage_paths = [ p for p in all_damage_file_paths if p not in train_damage_paths]\n",
    "print(\"test Damage image count: \", len(test_damage_paths))\n",
    "# ensure no overlap:\n",
    "overlap = [p for p in train_damage_paths if p in test_damage_paths]\n",
    "print(\"len of overlap: \", len(overlap))\n",
    "\n",
    "train_no_damage_paths = random.sample(all_no_damage_file_paths, int(len(all_no_damage_file_paths)*0.8))\n",
    "print(\"train No Damage image count: \", len(train_no_damage_paths))\n",
    "test_no_damage_paths = [ p for p in all_no_damage_file_paths if p not in train_no_damage_paths]\n",
    "print(\"test No Damage image count: \", len(test_no_damage_paths))\n",
    "# ensure no overlap:\n",
    "overlap = [p for p in train_no_damage_paths if p in test_no_damage_paths]\n",
    "print(\"len of overlap: \", len(overlap))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb8e4c0c-54d3-4763-9b43-50ab620a9001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in train/damage:  11336\n",
      "Files in train/no_damage:  2834\n",
      "Files in test/damage:  5721\n",
      "Files in test/no_damage:  1431\n"
     ]
    }
   ],
   "source": [
    "#ensure to copy the images to the directories\n",
    "import shutil\n",
    "for p in train_damage_paths:\n",
    "    shutil.copyfile(os.path.join('../data/project3/data_all_modified/damage', p), os.path.join(\"../data/project3/data_all_modified/data_split/train/damage\", p) )\n",
    "\n",
    "for p in test_damage_paths:\n",
    "    shutil.copyfile(os.path.join('../data/project3/data_all_modified/damage', p), os.path.join(\"../data/project3/data_all_modified/data_split/test/damage\", p) )\n",
    "\n",
    "for p in train_no_damage_paths:\n",
    "    shutil.copyfile(os.path.join('../data/project3/data_all_modified/no_damage', p), os.path.join(\"../data/project3/data_all_modified/data_split/train/no_damage\", p) )\n",
    "\n",
    "for p in test_no_damage_paths:\n",
    "    shutil.copyfile(os.path.join('../data/project3/data_all_modified/no_damage', p), os.path.join(\"../data/project3/data_all_modified/data_split/test/no_damage\", p) )\n",
    "\n",
    "# check counts:\n",
    "print(\"Files in train/damage: \", len(os.listdir(\"../data/project3/data_all_modified/data_split/train/damage\")))\n",
    "print(\"Files in train/no_damage: \", len(os.listdir(\"../data/project3/data_all_modified/data_split/test/damage\")))\n",
    "\n",
    "print(\"Files in test/damage: \", len(os.listdir(\"../data/project3/data_all_modified/data_split/train/no_damage\")))\n",
    "print(\"Files in test/no_damage: \", len(os.listdir(\"../data/project3/data_all_modified/data_split/test/no_damage\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020df377-ec38-40f6-b202-1ec305412c57",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f11d8a3-9af2-4183-a751-6e4d9e4f35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PIL to get image dimensions\n",
    "from PIL import Image\n",
    "\n",
    "def get_image_dimensions(image_path: str):\n",
    "    '''\n",
    "    Gets image dimensions through the input of a directory\n",
    "\n",
    "    Input: Takes string input of the directory path to an image\n",
    "\n",
    "    Output: Returns image dimensions of height & width pixels\n",
    "    '''\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            width, height = img.size\n",
    "            return width, height\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def check_images_same_size(directory: str):\n",
    "    '''\n",
    "    Ensures that all images within a directory are the same size\n",
    "\n",
    "    Input: Takes a string input of the desired directory to be check\n",
    "\n",
    "    Output: Prints a statement stating images are the same size & their dimensions,\n",
    "            or prints that the images have different sizes, or that there werent any valid images in\n",
    "            the directory.\n",
    "    '''\n",
    "    dimensions_set = set()  # To store unique dimensions of images\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".jpeg\"):\n",
    "            image_path = os.path.join(directory, filename)\n",
    "            dimensions = get_image_dimensions(image_path)\n",
    "            if dimensions:\n",
    "                dimensions_set.add(dimensions)\n",
    "\n",
    "    if len(dimensions_set) == 1:\n",
    "        dimensions = dimensions_set.pop()\n",
    "        print(\"All images are the same size.\")\n",
    "        print(f\"Image dimensions: {dimensions[0]}x{dimensions[1]} pixels\")\n",
    "    elif len(dimensions_set) > 1:\n",
    "        print(\"Images have different sizes.\")\n",
    "    else:\n",
    "        print(\"No valid images found in the directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e41c0d6-40ad-49b9-904b-bfbcc8ac4101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images are the same size.\n",
      "Image dimensions: 128x128 pixels\n",
      "All images are the same size.\n",
      "Image dimensions: 128x128 pixels\n"
     ]
    }
   ],
   "source": [
    "train_data_dmg = '../data/project3/data_all_modified/data_split/train/damage'\n",
    "train_data_no_dmg = '../data/project3/data_all_modified/data_split/train/no_damage'\n",
    "check_images_same_size(train_data_dmg)\n",
    "check_images_same_size(train_data_no_dmg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa908170-4e09-42fd-add6-495c81243f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.9.4-py3-none-any.whl (5.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.11/site-packages (from tensorflow_datasets) (2.1.0)\n",
      "Collecting click\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m219.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dm-tree\n",
      "  Downloading dm_tree-0.1.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.8/152.8 kB\u001b[0m \u001b[31m222.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting etils[enp,epath,etree]>=0.9.0\n",
      "  Downloading etils-1.8.0-py3-none-any.whl (156 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.1/156.1 kB\u001b[0m \u001b[31m301.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/site-packages (from tensorflow_datasets) (1.26.3)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/site-packages (from tensorflow_datasets) (4.23.4)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/site-packages (from tensorflow_datasets) (5.9.8)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/site-packages (from tensorflow_datasets) (2.31.0)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-1.14.0-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/site-packages (from tensorflow_datasets) (2.4.0)\n",
      "Collecting toml\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m258.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wrapt in /usr/local/lib/python3.11/site-packages (from tensorflow_datasets) (1.14.1)\n",
      "Collecting array-record>=0.5.0\n",
      "  Downloading array_record-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m219.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m298.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting importlib_resources\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (4.9.0)\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.11/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (3.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow_datasets) (2023.11.17)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/site-packages (from promise->tensorflow_datasets) (1.16.0)\n",
      "Collecting absl-py\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m289.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.63.0-py2.py3-none-any.whl (229 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.1/229.1 kB\u001b[0m \u001b[31m311.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting protobuf>=3.20\n",
      "  Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m311.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21483 sha256=fca7643be49509ffdb34acf4765cb018dc766950e81c3fadeee301eef701b8fe\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-4dkq4yio/wheels/74/05/89/d0909dd6ebad0a26f2b4dcb2499b1d65999c5b6ed416be7f85\n",
      "Successfully built promise\n",
      "Installing collected packages: dm-tree, tqdm, toml, protobuf, promise, importlib_resources, fsspec, etils, click, absl-py, googleapis-common-protos, tensorflow-metadata, array-record, tensorflow_datasets\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script tfds is installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed absl-py-1.4.0 array-record-0.5.1 click-8.1.7 dm-tree-0.1.8 etils-1.8.0 fsspec-2024.3.1 googleapis-common-protos-1.63.0 importlib_resources-6.4.0 promise-2.3 protobuf-3.20.3 tensorflow-metadata-1.14.0 tensorflow_datasets-4.9.4 toml-0.10.2 tqdm-4.66.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow_datasets --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00d20515-2474-4ff5-8f16-d97e5976d887",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 21:59:05.771940: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-09 21:59:05.806952: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-09 21:59:05.806985: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-09 21:59:05.808217: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-09 21:59:05.814566: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-09 21:59:05.815284: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-09 21:59:09.233620: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17057 files belonging to 2 classes.\n",
      "Using 13646 files for training.\n",
      "Using 3411 files for validation.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "# Now that we know the image dimensions\n",
    "train_data_dir = '../data/project3/data_all_modified/data_split/train'\n",
    "# Number of images we want to process at once\n",
    "batch_size = 64\n",
    "\n",
    "# Target image size (128 px by 128 px)\n",
    "img_height = 128\n",
    "img_width = 128\n",
    "train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "train_data_dir,\n",
    "validation_split=0.2,\n",
    "subset=\"both\",\n",
    "seed=123,\n",
    "image_size=(img_height, img_width),\n",
    "batch_size=batch_size\n",
    ")\n",
    "rescale = Rescaling(scale=1.0/255)\n",
    "train_rescale_ds = train_ds.map(lambda image,label:(rescale(image),label))\n",
    "val_rescale_ds = val_ds.map(lambda image,label:(rescale(image),label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6117878-a761-4209-9f0e-f5e3f4d64893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1de8e28e-3d39-4464-bb90-c8005ff2a815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17057 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data_dir = '../data/project3/data_all_modified/data_split/train'\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "img_height = 128\n",
    "img_width = 128\n",
    "\n",
    "# note that subset=\"training\", \"validation\", \"both\", and dictates what is returned\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "test_data_dir,\n",
    "seed=123,\n",
    "image_size=(img_height, img_width),\n",
    ")\n",
    "\n",
    "rescale = Rescaling(scale=1.0/255)\n",
    "test_rescale_ds = test_ds.map(lambda image,label:(rescale(image),label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e636d5c-d902-4ed8-8cf1-75e04f1dfcd9",
   "metadata": {},
   "source": [
    "## A Dense ANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d86299ce-91a1-4d48-afd3-47d513bcbb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a CNN with 3 alternating convolutional layers & pooling layers\n",
    "# with 2 dense hidden layers. Output layer has 3 classes & softmax activation\n",
    "\n",
    "from keras import layers, models, optimizers\n",
    "import pandas as pd\n",
    "\n",
    "# initialize sequential model\n",
    "model_cnn = models.Sequential()\n",
    "\n",
    "# Convolutional layer with 64 filters, and a kernel size of 3x3.\n",
    "# padding = 'same' gives the same output size as input_shape\n",
    "model_cnn.add(layers.Conv2D(64, (3, 3), activation='relu', padding=\"same\", input_shape=(img_height,img_width,3)))\n",
    "\n",
    "# Adding max pooling to reduce the size of output of first conv layer\n",
    "model_cnn.add(layers.MaxPooling2D((2, 2), padding = 'same'))\n",
    "\n",
    "# Second Convolutional layer with Pooling layer to reduce size\n",
    "model_cnn.add(layers.Conv2D(32, (3, 3), activation='relu', padding=\"same\"))\n",
    "model_cnn.add(layers.MaxPooling2D((2, 2), padding = 'same'))\n",
    "\n",
    "# Final convolutional layer & pooling to reduce size\n",
    "model_cnn.add(layers.Conv2D(32, (3, 3), activation='relu', padding=\"same\"))\n",
    "model_cnn.add(layers.MaxPooling2D((2, 2), padding = 'same'))\n",
    "\n",
    "# flattening the output of the conv layer after max pooling \n",
    "# makes it ready for creating dense connections\n",
    "model_cnn.add(layers.Flatten())\n",
    "\n",
    "# Adding a fully connected dense layer with 100 neurons\n",
    "model_cnn.add(layers.Dense(100, activation='relu'))\n",
    "\n",
    "# Adding a fully connected dense layer with 84 neurons\n",
    "model_cnn.add(layers.Dense(84, activation='relu'))\n",
    "\n",
    "# Adding the output layer with 2 neurons and \n",
    "# activation functions as softmax since this is a multi-class classification problem\n",
    "model_cnn.add(layers.Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d8e88ec-ffb2-4952-a66f-6e8d5e501a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 128, 128, 64)      1792      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 64, 64, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 64, 64, 32)        18464     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 32, 32, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 16, 16, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8192)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               819300    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 84)                8484      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 170       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 857458 (3.27 MB)\n",
      "Trainable params: 857458 (3.27 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "# RMSprop (Root Mean Square Propagation) is commonly used in training deep neural networks.\n",
    "model_cnn.compile(optimizer=optimizers.RMSprop(learning_rate=1e-4), \n",
    "                  loss='sparse_categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Generating the summary of the model\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94882936-3220-4c4c-be23-d4d6110a5166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "214/214 [==============================] - 107s 491ms/step - loss: 0.5947 - accuracy: 0.7092 - val_loss: 0.6913 - val_accuracy: 0.6722\n",
      "Epoch 2/20\n",
      "214/214 [==============================] - 106s 494ms/step - loss: 0.4646 - accuracy: 0.7954 - val_loss: 0.4194 - val_accuracy: 0.7986\n",
      "Epoch 3/20\n",
      "214/214 [==============================] - 106s 497ms/step - loss: 0.4091 - accuracy: 0.8253 - val_loss: 0.3344 - val_accuracy: 0.8690\n",
      "Epoch 4/20\n",
      "214/214 [==============================] - 103s 483ms/step - loss: 0.3589 - accuracy: 0.8507 - val_loss: 0.2929 - val_accuracy: 0.8898\n",
      "Epoch 5/20\n",
      "214/214 [==============================] - 95s 446ms/step - loss: 0.3168 - accuracy: 0.8723 - val_loss: 0.2493 - val_accuracy: 0.9006\n",
      "Epoch 6/20\n",
      "214/214 [==============================] - 95s 443ms/step - loss: 0.2782 - accuracy: 0.8890 - val_loss: 0.2520 - val_accuracy: 0.9050\n",
      "Epoch 7/20\n",
      "214/214 [==============================] - 99s 462ms/step - loss: 0.2357 - accuracy: 0.9089 - val_loss: 0.2595 - val_accuracy: 0.9044\n",
      "Epoch 8/20\n",
      "214/214 [==============================] - 95s 445ms/step - loss: 0.2150 - accuracy: 0.9156 - val_loss: 0.2741 - val_accuracy: 0.8909\n",
      "Epoch 9/20\n",
      "214/214 [==============================] - 98s 458ms/step - loss: 0.1943 - accuracy: 0.9258 - val_loss: 0.1948 - val_accuracy: 0.9299\n",
      "Epoch 10/20\n",
      "214/214 [==============================] - 101s 471ms/step - loss: 0.1811 - accuracy: 0.9300 - val_loss: 0.2213 - val_accuracy: 0.9141\n",
      "Epoch 11/20\n",
      "214/214 [==============================] - 104s 486ms/step - loss: 0.1697 - accuracy: 0.9338 - val_loss: 0.1574 - val_accuracy: 0.9376\n",
      "Epoch 12/20\n",
      "214/214 [==============================] - 103s 479ms/step - loss: 0.1570 - accuracy: 0.9382 - val_loss: 0.1382 - val_accuracy: 0.9420\n",
      "Epoch 13/20\n",
      "214/214 [==============================] - 102s 476ms/step - loss: 0.1478 - accuracy: 0.9430 - val_loss: 0.1329 - val_accuracy: 0.9452\n",
      "Epoch 14/20\n",
      "214/214 [==============================] - 99s 461ms/step - loss: 0.1389 - accuracy: 0.9453 - val_loss: 0.1419 - val_accuracy: 0.9446\n",
      "Epoch 15/20\n",
      "214/214 [==============================] - 98s 457ms/step - loss: 0.1305 - accuracy: 0.9480 - val_loss: 0.2406 - val_accuracy: 0.9018\n",
      "Epoch 16/20\n",
      "214/214 [==============================] - 93s 436ms/step - loss: 0.1210 - accuracy: 0.9532 - val_loss: 0.1716 - val_accuracy: 0.9285\n",
      "Epoch 17/20\n",
      "214/214 [==============================] - 92s 429ms/step - loss: 0.1118 - accuracy: 0.9556 - val_loss: 0.1724 - val_accuracy: 0.9282\n",
      "Epoch 18/20\n",
      "214/214 [==============================] - 94s 441ms/step - loss: 0.1062 - accuracy: 0.9580 - val_loss: 0.2498 - val_accuracy: 0.9018\n",
      "Epoch 19/20\n",
      "214/214 [==============================] - 90s 420ms/step - loss: 0.1049 - accuracy: 0.9590 - val_loss: 0.0939 - val_accuracy: 0.9592\n",
      "Epoch 20/20\n",
      "214/214 [==============================] - 92s 431ms/step - loss: 0.0975 - accuracy: 0.9606 - val_loss: 0.1157 - val_accuracy: 0.9549\n"
     ]
    }
   ],
   "source": [
    "#fit the model from image generator\n",
    "history = model_cnn.fit(\n",
    "            train_rescale_ds,\n",
    "            batch_size=64,\n",
    "            epochs=20,\n",
    "            validation_data=val_rescale_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a39d601-04b7-47f7-9f01-13036ab95e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.10109329223632812\n",
      "Test Accuracy: 0.959254264831543\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model_cnn.evaluate(test_rescale_ds, verbose = 0)\n",
    "# validation accuracy\n",
    "print(f'Test Loss: {test_loss}')\n",
    "# test accuracy\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453435fa-8a9e-46df-8cf7-78c24ffc69f7",
   "metadata": {},
   "source": [
    "## LeNet-5 Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3a3d7a3-140d-47c2-9ad3-47cd4473c26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 126, 126, 6)       168       \n",
      "                                                                 \n",
      " average_pooling2d (Average  (None, 63, 63, 6)         0         \n",
      " Pooling2D)                                                      \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 61, 61, 16)        880       \n",
      "                                                                 \n",
      " average_pooling2d_1 (Avera  (None, 30, 30, 16)        0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 14400)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 120)               1728120   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 84)                10164     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 170       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1739502 (6.64 MB)\n",
      "Trainable params: 1739502 (6.64 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_lenet5 = models.Sequential()\n",
    "\n",
    "# Layer 1: Convolutional layer with 6 filters of size 3x3, followed by average pooling\n",
    "model_lenet5.add(layers.Conv2D(6, kernel_size=(3, 3), activation='relu', input_shape=(img_height,img_width,3)))\n",
    "model_lenet5.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 2: Convolutional layer with 16 filters of size 3x3, followed by average pooling\n",
    "model_lenet5.add(layers.Conv2D(16, kernel_size=(3, 3), activation='relu'))\n",
    "model_lenet5.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the feature maps to feed into fully connected layers\n",
    "model_lenet5.add(layers.Flatten())\n",
    "\n",
    "# Layer 3: Fully connected layer with 120 neurons\n",
    "model_lenet5.add(layers.Dense(120, activation='relu'))\n",
    "\n",
    "# Layer 4: Fully connected layer with 84 neurons\n",
    "model_lenet5.add(layers.Dense(84, activation='relu'))\n",
    "\n",
    "# Output layer: Fully connected layer with num_classes neurons (e.g., 2 )\n",
    "model_lenet5.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model_lenet5.compile(optimizer=optimizers.RMSprop(learning_rate=1e-4), \n",
    "                     loss='sparse_categorical_crossentropy', \n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "# Generating the summary of the model\n",
    "model_lenet5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05808305-ef43-439f-bf38-478b0e7b6dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "214/214 [==============================] - 19s 84ms/step - loss: 0.6060 - accuracy: 0.6908 - val_loss: 0.5792 - val_accuracy: 0.7227\n",
      "Epoch 2/20\n",
      "214/214 [==============================] - 18s 84ms/step - loss: 0.4675 - accuracy: 0.8057 - val_loss: 0.4176 - val_accuracy: 0.8050\n",
      "Epoch 3/20\n",
      "214/214 [==============================] - 18s 82ms/step - loss: 0.3774 - accuracy: 0.8576 - val_loss: 0.4992 - val_accuracy: 0.7710\n",
      "Epoch 4/20\n",
      "214/214 [==============================] - 18s 82ms/step - loss: 0.3449 - accuracy: 0.8717 - val_loss: 0.3520 - val_accuracy: 0.8593\n",
      "Epoch 5/20\n",
      "214/214 [==============================] - 18s 85ms/step - loss: 0.3253 - accuracy: 0.8815 - val_loss: 0.3046 - val_accuracy: 0.8930\n",
      "Epoch 6/20\n",
      "214/214 [==============================] - 19s 89ms/step - loss: 0.3087 - accuracy: 0.8881 - val_loss: 0.3153 - val_accuracy: 0.8807\n",
      "Epoch 7/20\n",
      "214/214 [==============================] - 18s 82ms/step - loss: 0.2878 - accuracy: 0.8950 - val_loss: 0.2889 - val_accuracy: 0.8971\n",
      "Epoch 8/20\n",
      "214/214 [==============================] - 18s 82ms/step - loss: 0.2732 - accuracy: 0.8991 - val_loss: 0.4335 - val_accuracy: 0.8273\n",
      "Epoch 9/20\n",
      "214/214 [==============================] - 18s 85ms/step - loss: 0.2598 - accuracy: 0.9043 - val_loss: 0.2547 - val_accuracy: 0.9024\n",
      "Epoch 10/20\n",
      "214/214 [==============================] - 18s 83ms/step - loss: 0.2455 - accuracy: 0.9078 - val_loss: 0.3478 - val_accuracy: 0.8619\n",
      "Epoch 11/20\n",
      "214/214 [==============================] - 18s 82ms/step - loss: 0.2323 - accuracy: 0.9111 - val_loss: 0.2336 - val_accuracy: 0.9135\n",
      "Epoch 12/20\n",
      "214/214 [==============================] - 17s 81ms/step - loss: 0.2219 - accuracy: 0.9162 - val_loss: 0.2956 - val_accuracy: 0.8833\n",
      "Epoch 13/20\n",
      "214/214 [==============================] - 18s 82ms/step - loss: 0.2104 - accuracy: 0.9216 - val_loss: 0.2243 - val_accuracy: 0.9135\n",
      "Epoch 14/20\n",
      "214/214 [==============================] - 18s 83ms/step - loss: 0.1999 - accuracy: 0.9242 - val_loss: 0.2158 - val_accuracy: 0.9182\n",
      "Epoch 15/20\n",
      "214/214 [==============================] - 19s 87ms/step - loss: 0.1923 - accuracy: 0.9275 - val_loss: 0.2250 - val_accuracy: 0.9082\n",
      "Epoch 16/20\n",
      "214/214 [==============================] - 20s 92ms/step - loss: 0.1820 - accuracy: 0.9332 - val_loss: 0.2154 - val_accuracy: 0.9176\n",
      "Epoch 17/20\n",
      "214/214 [==============================] - 19s 86ms/step - loss: 0.1757 - accuracy: 0.9327 - val_loss: 0.2895 - val_accuracy: 0.8839\n",
      "Epoch 18/20\n",
      "214/214 [==============================] - 17s 81ms/step - loss: 0.1663 - accuracy: 0.9373 - val_loss: 0.1930 - val_accuracy: 0.9255\n",
      "Epoch 19/20\n",
      "214/214 [==============================] - 18s 82ms/step - loss: 0.1591 - accuracy: 0.9384 - val_loss: 0.2136 - val_accuracy: 0.9167\n",
      "Epoch 20/20\n",
      "214/214 [==============================] - 18s 83ms/step - loss: 0.1525 - accuracy: 0.9417 - val_loss: 0.2023 - val_accuracy: 0.9214\n"
     ]
    }
   ],
   "source": [
    "history = model_lenet5.fit(\n",
    "            train_rescale_ds,\n",
    "            batch_size=64,\n",
    "            epochs=20,\n",
    "            validation_data=val_rescale_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af0de671-fb84-41e0-afa1-7ae95de50dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.16592474281787872\n",
      "Test Accuracy: 0.9392038583755493\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model_lenet5.evaluate(test_rescale_ds, verbose = 0)\n",
    "# validation accuracy\n",
    "print(f'Test Loss: {test_loss}')\n",
    "# test accuracy\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edc1cb8-f3a1-4c00-973c-f57d70afd4c8",
   "metadata": {},
   "source": [
    "## Modified LeNet-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c463cf1-9b2d-481b-afb9-04854632bf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17057 files belonging to 2 classes.\n",
      "Using 13646 files for training.\n",
      "Using 3411 files for validation.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "# Now that we know the image dimensions\n",
    "train_data_dir = 'data/project3/data_all_modified/data_split/train'\n",
    "# Number of images we want to process at once\n",
    "batch_size = 64\n",
    "\n",
    "# Target image size (128 px by 128 px)\n",
    "img_height = 128\n",
    "img_width = 128\n",
    "train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "train_data_dir,\n",
    "validation_split=0.2,\n",
    "subset=\"both\",\n",
    "seed=123,\n",
    "image_size=(img_height, img_width),\n",
    "batch_size=batch_size\n",
    ")\n",
    "rescale = Rescaling(scale=1.0/255)\n",
    "train_rescale_ds = train_ds.map(lambda image,label:(rescale(image),label))\n",
    "val_rescale_ds = val_ds.map(lambda image,label:(rescale(image),label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd245330-1c8c-4aff-80dc-b17c39a59d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17057 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data_dir = 'data/project3/data_all_modified/data_split/train'\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "img_height = 128\n",
    "img_width = 128\n",
    "\n",
    "# note that subset=\"training\", \"validation\", \"both\", and dictates what is returned\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "test_data_dir,\n",
    "seed=123,\n",
    "image_size=(img_height, img_width),\n",
    ")\n",
    "\n",
    "rescale = Rescaling(scale=1.0/255)\n",
    "test_rescale_ds = test_ds.map(lambda image,label:(rescale(image),label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "337ad6a2-d07b-4f00-9af1-45bd2b547c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_5 (Conv2D)           (None, 126, 126, 6)       168       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 63, 63, 6)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 61, 61, 32)        1760      \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 30, 30, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 28, 28, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 14, 14, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPoolin  (None, 6, 6, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 4, 4, 128)         147584    \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 2, 2, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 505546 (1.93 MB)\n",
      "Trainable params: 505546 (1.93 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "import pandas as pd\n",
    "\n",
    "model_lenet5 = models.Sequential()\n",
    "# Layer 1: Convolutional layer with 6 filters of size 3x3, followed by Max pooling\n",
    "model_lenet5.add(layers.Conv2D(6, kernel_size=(3, 3), activation='relu', input_shape=(img_height,img_width,3)))\n",
    "model_lenet5.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "# Layer 2: Convolutional layer with 32 filters of size 3x3, followed by Max pooling\n",
    "model_lenet5.add(layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(img_height,img_width,3)))\n",
    "model_lenet5.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "# Layer 3: Convolutional layer with 64 filters of size 3x3, followed by Max pooling\n",
    "model_lenet5.add(layers.Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(img_height,img_width,3)))\n",
    "model_lenet5.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "# Layer 4: Convolutional layer with 128 filters of size 3x3, followed by Max pooling\n",
    "model_lenet5.add(layers.Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(img_height,img_width,3)))\n",
    "model_lenet5.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "# Layer 5: Convolutional layer with 128 filters of size 3x3, followed by Max pooling\n",
    "model_lenet5.add(layers.Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(img_height,img_width,3)))\n",
    "model_lenet5.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "# flattening the output of the conv layer after max pooling \n",
    "model_lenet5.add(layers.Flatten())\n",
    "#Dropout Layer\n",
    "model_lenet5.add(layers.Dropout(0.2))\n",
    "# Layer 8: Fully connected layer with 512 neurons\n",
    "model_lenet5.add(layers.Dense(512, activation='relu'))\n",
    "# Output layer: Fully connected layer with num_classes neurons (e.g., 2 )\n",
    "model_lenet5.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model_lenet5.compile(optimizer=optimizers.RMSprop(learning_rate=1e-4), \n",
    "                     loss='sparse_categorical_crossentropy', \n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "# Generating the summary of the model\n",
    "model_lenet5.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3d46ddf-f1ff-4496-92bc-a475d10e2840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "214/214 [==============================] - 29s 131ms/step - loss: 0.6243 - accuracy: 0.6716 - val_loss: 0.5944 - val_accuracy: 0.6693\n",
      "Epoch 2/20\n",
      "214/214 [==============================] - 30s 141ms/step - loss: 0.4743 - accuracy: 0.7796 - val_loss: 0.4676 - val_accuracy: 0.7684\n",
      "Epoch 3/20\n",
      "214/214 [==============================] - 33s 154ms/step - loss: 0.4116 - accuracy: 0.8259 - val_loss: 0.4123 - val_accuracy: 0.8150\n",
      "Epoch 4/20\n",
      "214/214 [==============================] - 32s 147ms/step - loss: 0.3730 - accuracy: 0.8484 - val_loss: 0.3524 - val_accuracy: 0.8575\n",
      "Epoch 5/20\n",
      "214/214 [==============================] - 31s 144ms/step - loss: 0.3334 - accuracy: 0.8642 - val_loss: 0.3662 - val_accuracy: 0.8467\n",
      "Epoch 6/20\n",
      "214/214 [==============================] - 31s 147ms/step - loss: 0.2979 - accuracy: 0.8816 - val_loss: 0.2418 - val_accuracy: 0.9024\n",
      "Epoch 7/20\n",
      "214/214 [==============================] - 32s 151ms/step - loss: 0.2635 - accuracy: 0.8953 - val_loss: 0.2324 - val_accuracy: 0.9126\n",
      "Epoch 8/20\n",
      "214/214 [==============================] - 32s 147ms/step - loss: 0.2408 - accuracy: 0.9048 - val_loss: 0.2287 - val_accuracy: 0.9123\n",
      "Epoch 9/20\n",
      "214/214 [==============================] - 28s 132ms/step - loss: 0.2230 - accuracy: 0.9105 - val_loss: 0.1895 - val_accuracy: 0.9232\n",
      "Epoch 10/20\n",
      "214/214 [==============================] - 30s 138ms/step - loss: 0.2106 - accuracy: 0.9172 - val_loss: 0.2349 - val_accuracy: 0.9077\n",
      "Epoch 11/20\n",
      "214/214 [==============================] - 30s 138ms/step - loss: 0.1992 - accuracy: 0.9195 - val_loss: 0.1953 - val_accuracy: 0.9214\n",
      "Epoch 12/20\n",
      "214/214 [==============================] - 30s 139ms/step - loss: 0.1873 - accuracy: 0.9246 - val_loss: 0.2326 - val_accuracy: 0.9003\n",
      "Epoch 13/20\n",
      "214/214 [==============================] - 30s 139ms/step - loss: 0.1757 - accuracy: 0.9307 - val_loss: 0.1429 - val_accuracy: 0.9422\n",
      "Epoch 14/20\n",
      "214/214 [==============================] - 30s 139ms/step - loss: 0.1627 - accuracy: 0.9358 - val_loss: 0.2023 - val_accuracy: 0.9144\n",
      "Epoch 15/20\n",
      "214/214 [==============================] - 28s 129ms/step - loss: 0.1588 - accuracy: 0.9362 - val_loss: 0.1383 - val_accuracy: 0.9393\n",
      "Epoch 16/20\n",
      "214/214 [==============================] - 28s 129ms/step - loss: 0.1532 - accuracy: 0.9384 - val_loss: 0.1804 - val_accuracy: 0.9232\n",
      "Epoch 17/20\n",
      "214/214 [==============================] - 32s 147ms/step - loss: 0.1447 - accuracy: 0.9419 - val_loss: 0.1450 - val_accuracy: 0.9387\n",
      "Epoch 18/20\n",
      "214/214 [==============================] - 32s 148ms/step - loss: 0.1410 - accuracy: 0.9433 - val_loss: 0.3856 - val_accuracy: 0.8291\n",
      "Epoch 19/20\n",
      "214/214 [==============================] - 30s 139ms/step - loss: 0.1375 - accuracy: 0.9445 - val_loss: 0.1396 - val_accuracy: 0.9452\n",
      "Epoch 20/20\n",
      "214/214 [==============================] - 30s 141ms/step - loss: 0.1323 - accuracy: 0.9479 - val_loss: 0.1151 - val_accuracy: 0.9525\n"
     ]
    }
   ],
   "source": [
    "history = model_lenet5.fit(\n",
    "            train_rescale_ds,\n",
    "            batch_size=64,\n",
    "            epochs=20,\n",
    "            validation_data=val_rescale_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cf708be-1f90-4c2d-a161-f90880de5a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.116956427693367\n",
      "Test Accuracy: 0.9566991925239563\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model_lenet5.evaluate(test_rescale_ds, verbose = 0)\n",
    "# validation accuracy\n",
    "print(f'Test Loss: {test_loss}')\n",
    "# test accuracy\n",
    "print(f'Test Accuracy: {test_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc37bd9c-6386-40af-acf7-a736caf7c225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
